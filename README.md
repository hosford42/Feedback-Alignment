# Feedback-Alignment
Feedback alignment is a backpropagation modification where the next layer weights become a fixed random matrix. [Lillicrap et al](https://www.nature.com/articles/ncomms13276) shows a FA is a regularizer where the next layer weights must learn to orient within 90 degree in order to perform effective training. The main obstacle of FA is to prove of general convergence under nonlinear dynamics.
